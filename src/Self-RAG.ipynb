{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Initialize the LLM (Large Language Model) with the specified model and data type\n",
    "model = LLM(\"selfrag/selfrag_llama2_7b\", dtype=\"half\")\n",
    "\n",
    "# Define sampling parameters for text generation\n",
    "sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=100, skip_special_tokens=False)\n",
    "\n",
    "# Define a function to format prompts, including an instruction and an optional paragraph for retrieval\n",
    "def format_prompt(input, paragraph=None):\n",
    "  prompt = \"### Instruction:\\n{0}\\n\\n### Response:\\n\".format(input)\n",
    "  if paragraph is not None:\n",
    "    prompt += \"[Retrieval]<paragraph>{0}</paragraph>\".format(paragraph)\n",
    "  return prompt\n",
    "\n",
    "# Define two queries for the model to generate responses for\n",
    "query_1 = \"Leave odd one out: twitter, instagram, whatsapp.\"\n",
    "query_2 = \"Can you tell me the difference between llamas and alpacas?\"\n",
    "queries = [query_1, query_2]\n",
    "\n",
    "# Generate responses for the queries\n",
    "preds = model.generate([format_prompt(query) for query in queries], sampling_params)\n",
    "\n",
    "# Print the model's predictions for each query\n",
    "for pred in preds:\n",
    "  print(\"\\n\\nModel prediction: {0}\".format(pred.outputs[0].text))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
